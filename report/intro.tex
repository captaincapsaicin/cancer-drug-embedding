\section{Introduction}

There is great demand for new compounds that could be used as drugs to treat disease. Drugs may be precisely targeted to subpopulations, or that can circumvent disease resistance. In addition, drugs may have many negative side effects: can similar therapeutic effects be found in different compounds with fewer side effects?

Testing massive libraries with hundreds of thousands of compounds for their biological effects is time consuming and expensive. To reduce this cost, recent research computationally explores the space of compounds using generative models (e.g. variational autoencoders). The parameter space of these generative models can be optimized over to maximize or specify the output of a predictor function (e.g. drug effect) \cite{Brookes2018} \cite{Gomez-Bombarelli2018}. Chemistry suffers from a problem similar to biology, natural language, and natural images: there are massive unlabeled datasets, and very few corresponding labeled datasets. For this reason, unsupervised and semi-supervised modeling approaches hold promise for helping models predict function from chemical structure.

Another promising avenue for meaningfully summarizing structural similarity follows research from natural language processing, and the learning of word and document embeddings \cite{Goldberg2014}. By drawing the analogy of graphical substructures (rooted subgraphs) within a complete graph as ``words'' within a ``document'', similar intuition can be applied to learning $\delta$-dimensional continuous embeddings for arbitrary graph structures, deemed \textit{graph2vec} \cite{Narayanan}. Graphs with similar substructures are similar graphs, and are close (in a Euclidean distance sense) in the embedding space. Since molecules can be represented as graphs, \textit{graph2vec} embeddings can take advantage of large, unlabeled molecular datasets to learn rich embeddings that capture structural similarity between molecules. This also circumvents one of the problems with learning molecular embeddings via complicated strings written in Simplified Molecular-Input Line-Entry System (SMILES) as in \cite{Gomez-Bombarelli2018}. \textit{graph2vec} does not need to learn the complex chemical syntax of SMILES. ``Word'' embeddings for n-grams of amino acids and nucleotides have already shown promise for being able to meaningfully capture biophysical characteristics of proteins and DNA sequences \cite{Hamid2018} \cite{Asgari2015}.

In this paper we examine the use of \textit{graph2vec} as a means for refining compound screens.
