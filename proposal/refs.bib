@techreport{Fukuyama2017,
abstract = {When working with large biological data sets, exploratory analysis is an important first step for understanding the latent structure and for generating hypotheses to be tested in subsequent analyses. However, when the number of variables is large compared to the number of samples, standard methods such as principal components analysis give results which are unstable and difficult to interpret. To mitigate these problems, we have developed a method which allows the analyst to incorporate side information about the relationships between the variables in a way that encourages similar variables to have similar loadings on the principal axes. This leads to a low-dimensional representation of the samples which both describes the latent structure and which has axes which are interpretable in terms of groups of closely related variables. The method is derived by putting a prior encoding the relationships between the variables on the data and following through the analysis on the posterior distributions of the samples. We show that our method does well at reconstructing true latent structure in simulated data and we also demonstrate the method on a dataset investigating the effects of antibiotics on the composition of bacteria in the human gut.},
archivePrefix = {arXiv},
arxivId = {arXiv:1702.00501v1},
author = {Fukuyama, Julia},
eprint = {arXiv:1702.00501v1},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Fukuyama - 2017 - Adaptive gPCA A method for structured dimensionality reduction(2).pdf:pdf},
title = {{Adaptive gPCA: A method for structured dimensionality reduction}},
url = {https://arxiv.org/pdf/1702.00501.pdf},
year = {2017}
}
@article{Narayanan,
abstract = {Recent works on representation learning for graph struc-tured data predominantly focus on learning distributed representations of graph substructures such as nodes and sub-graphs. However, many graph analytics tasks such as graph classification and clustering require representing entire graphs as fixed length feature vectors. While the afore-mentioned approaches are naturally unequipped to learn such representations, graph kernels remain as the most effective way of obtaining them. However, these graph kernels use handcrafted features (e.g., shortest paths, graphlets, etc.) and hence are hampered by problems such as poor generalization. To address this limitation, in this work, we propose a neural embedding framework named graph2vec to learn data-driven distributed representations of arbitrary sized graphs. graph2vec's embeddings are learnt in an un-supervised manner and are task agnostic. Hence, they could be used for any downstream task such as graph classification, clustering and even seeding supervised representation learning approaches. Our experiments on several benchmark and large real-world datasets show that graph2vec achieves significant improvements in classification and clustering accuracies over substructure representation learning approaches and are competitive with state-of-the-art graph kernels.},
archivePrefix = {arXiv},
arxivId = {1707.05005v1},
author = {Narayanan, Annamalai and Chandramohan, Mahinthan and Venkatesan, Rajasekar and Chen, Lihui and Liu, Yang and Jaiswal, Shantanu},
doi = {10.1145/1235},
eprint = {1707.05005v1},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Narayanan et al. - Unknown - graph2vec Learning Distributed Representations of Graphs.pdf:pdf},
isbn = {9781450321389},
keywords = {Deep Learning,Graph Kernels,Representation Learning},
title = {{graph2vec: Learning Distributed Representations of Graphs}},
url = {https://arxiv.org/pdf/1707.05005.pdf}
}
@techreport{Duvenaud,
abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better pre-dictive performance on a variety of tasks.},
author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and G{\'{o}}mez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'{a}}n and Adams, Ryan P},
file = {::},
title = {{Convolutional Networks on Graphs for Learning Molecular Fingerprints}},
url = {http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf}
}
@article{Kang2016,
abstract = {A method to systematically identify optimal biomarkers improves high-content screening for drug candidates.},
author = {Kang, Jungseog and Hsu, Chien-Hsiang and Wu, Qi and Liu, Shanshan and Coster, Adam D and Posner, Bruce A and Altschuler, Steven J and Wu, Lani F},
doi = {10.1038/nbt.3419},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Kang et al. - 2016 - Improving drug discovery with high-content phenotypic screens by systematic selection of reporter cell lines(3).pdf:pdf},
issn = {1087-0156},
journal = {Nature Biotechnology},
keywords = {High,Image processing,throughput screening},
month = {jan},
number = {1},
pages = {70--77},
publisher = {Nature Publishing Group},
title = {{Improving drug discovery with high-content phenotypic screens by systematic selection of reporter cell lines}},
url = {http://www.nature.com/articles/nbt.3419},
volume = {34},
year = {2016}
}
@article{Brookes2018,
abstract = {We present a probabilistic modeling framework and adaptive sampling algorithm wherein unsupervised generative models are combined with black box predictive models to tackle the problem of input design. In input design, one is given one or more stochastic "oracle" predictive functions, each of which maps from the input design space (e.g. DNA sequences or images) to a distribution over a property of interest (e.g. protein fluorescence or image content). Given such stochastic oracles, the problem is to find an input that is expected to maximize one or more properties, or to achieve a specified value of one or more properties, or any combination thereof. We demonstrate experimentally that our approach substantially outperforms other recently presented methods for tackling a specific version of this problem, namely, maximization when the oracle is assumed to be deterministic and unbiased. We also demonstrate that our method can tackle more general versions of the problem.},
archivePrefix = {arXiv},
arxivId = {1810.03714},
author = {Brookes, David H. and Listgarten, Jennifer},
eprint = {1810.03714},
file = {::},
month = {oct},
title = {{Design by adaptive sampling}},
url = {http://arxiv.org/abs/1810.03714},
year = {2018}
}
@article{Goldberg2014,
abstract = {The word2vec software of Tomas Mikolov and colleagues 1 has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers [1, 2]. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in " Dis-tributed Representations of Words and Phrases and their Compositionality " by The departure point of the paper is the skip-gram model. In this model we are given a corpus of words w and their contexts c. We consider the conditional probabilities p(c|w), and given a corpus T ext, the goal is to set the parameters $\theta$ of p(c|w; $\theta$) so as to maximize the corpus probability: arg max},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.3722v1},
author = {Goldberg, Yoav and Levy, Omer and Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {arXiv:1402.3722v1},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Goldberg et al. - 2014 - word2vec Explained Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method The skip-gram model.pdf:pdf},
keywords = {()},
title = {{word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method The skip-gram model}},
url = {https://arxiv.org/pdf/1402.3722.pdf},
year = {2014}
}
@article{Gomez-Bombarelli2018,
abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous represent...},
author = {G{\'{o}}mez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and S{\'{a}}nchez-Lengeling, Benjam{\'{i}}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Al{\'{a}}n},
doi = {10.1021/acscentsci.7b00572},
issn = {2374-7943},
journal = {ACS Central Science},
month = {feb},
number = {2},
pages = {268--276},
publisher = {American Chemical Society},
title = {{Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules}},
url = {http://pubs.acs.org/doi/10.1021/acscentsci.7b00572},
volume = {4},
year = {2018}
}
@techreport{Lamb,
abstract = {To pursue a systematic approach to the discovery of functional connections among diseases, genetic perturbation, and drug action, we have created the first installment of a reference collection of gene-expression profiles from cultured human cells treated with bioactive small molecules, together with pattern-matching software to mine these data. We demonstrate that this ''Connectivity Map'' resource can be used to find connections among small molecules sharing a mechanism of action, chemicals and physiological processes, and diseases and drugs. These results indicate the feasibility of the approach and suggest the value of a large-scale community Connectivity Map project.},
author = {Lamb, Justin and Crawford, Emily D and Peck, David and Modell, Joshua W and Blat, Irene C and Wrobel, Matthew J and Lerner, Jim and Brunet, Jean-Philippe and Subramanian, Aravind and Ross, Kenneth N and Reich, Michael and Hieronymus, Haley and Wei, Guo and Armstrong, Scott A and Haggarty, Stephen J and Clemons, Paul A and Wei, Ru and Carr, Steven A and Lander, Eric S and Golub, Todd R},
file = {::},
title = {{The Connectivity Map: Using Gene-Expression Signatures to Connect Small Molecules, Genes, and Disease}},
url = {http://science.sciencemag.org/}
}
@article{Hamid2018,
abstract = {Antibiotic resistance is a major public health crisis, and finding new sources of antimicrobial drugs is crucial to solving it. Bacteriocins, which are bacterially-produced antimicrobial peptide products, are candidates for broadening our pool of antimicrobials. The discovery of new bacteriocins by genomic mining is hampered by their sequences' low complexity and high variance, which frustrates sequence similarity-based searches. Here we use word embeddings of protein sequences to represent bacteriocins, and subsequently apply Recurrent Neural Networks and Support Vector Machines to predict novel bacteriocins from protein sequences without using sequence similarity. We developed a word embedding method that accounts for sequence order, providing a better classification than a simple summation of the same word embeddings. We use the Uniprot/TrEMBL database to acquire the word embeddings taking advantage of a large volume of unlabeled data. Our method predicts, with a high probability, six yet unknown putative bacteriocins in Lactobacillus. Generalized, the representation of sequences with word embeddings preserving sequence order information can be applied to protein classification problems for which sequence homology cannot be used.},
author = {Hamid, Md Nafiz and Friedberg, Iddo},
doi = {10.1101/255505},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Hamid, Friedberg - 2018 - Identifying Antimicrobial Peptides using Word Embedding with Deep Recurrent Neural Networks.pdf:pdf},
journal = {bioRxiv},
month = {jan},
pages = {255505},
publisher = {Cold Spring Harbor Laboratory},
title = {{Identifying Antimicrobial Peptides using Word Embedding with Deep Recurrent Neural Networks}},
url = {https://www.biorxiv.org/content/early/2018/01/29/255505},
year = {2018}
}
@article{Asgari2015,
abstract = {We introduce a new representation and feature extraction method for biological sequences. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. In the present paper, we focus on protein-vectors that can be utilized in a wide array of bioinformatics investigations such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. In this method, we adopt artificial neural network approaches and represent a protein sequence with a single dense n-dimensional vector. To evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families, where an average family classification accuracy of 93{\%}+-0.06{\%} is obtained, outperforming existing family classification methods. In addition, we use ProtVec representation to predict disordered proteins from structured proteins. Two databases of disordered sequences are used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8{\%} accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0{\%} accuracy. These results indicate that by only providing sequence data for various proteins into this model, accurate information about protein structure can be determined.},
archivePrefix = {arXiv},
arxivId = {1503.05140},
author = {Asgari, Ehsaneddin and Mofrad, Mohammad R.K.},
doi = {10.1371/journal.pone.0141287},
editor = {Kobeissy, Firas H},
eprint = {1503.05140},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Asgari, Mofrad - 2015 - Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics.pdf:pdf},
isbn = {19326203 (Electronic)},
issn = {19326203},
journal = {PLoS ONE},
month = {nov},
number = {11},
pages = {e0141287},
pmid = {26555596},
publisher = {Public Library of Science},
title = {{Continuous distributed representation of biological sequences for deep proteomics and genomics}},
url = {http://dx.plos.org/10.1371/journal.pone.0141287},
volume = {10},
year = {2015}
}
